{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "We are a team of political advisors, and our job is to provide insights to political and business groups on the current political landscape, and how they should steer their policies and strategies.\n",
    "\n",
    "Reddit posts and comments of two dominant political groups, Democrats and Conservatives, were downloaded and studied. The goal of this project is to build a classifier on which subreddit a given post came from. In addition, by researching the popular texts among Republican and Democrat supporters, we expect to extract key words that implies major issues facing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Reddit is the only data source. About 1000 posts and related comments and selftexts from each subreddit groups are downloaded by webscrapping. \n",
    "\n",
    "Two subreddit groups are used:\n",
    "- r/Conservative\n",
    "- r/democrats\n",
    "\n",
    "### Data Preprocessing\n",
    "The data is then preprocessed in the following steps to remove undesirable features from natural language processing perspective:\n",
    "- remove non-characters for saving texts in csv properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Downloading data - Democrats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the Democrats' posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First level of crawling - list of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url - Reddit API via .json\n",
    "url = \"https://www.reddit.com/r/democrats.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to to make HTML requests to Reddit to receive the information in json format\n",
    "# Each page has ~25 posts so the loop will go through 40 times to get ~1000 posts\n",
    "# This page contains information such as posts' name (similar to id), title, url of individual post etc\n",
    "# Request status 200 means success\n",
    "# Add a latency of 0.2 second after each request\n",
    "\n",
    "def extract_posts(url):\n",
    "    \n",
    "    headers = {\"User-agent\": \"Bot DSI\"}\n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    # loop for 40 times\n",
    "    for i in range(40):\n",
    "        print(i)\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            # Field \"after\" refers to the last name (id) of the post on the current page\n",
    "            params = {\"after\": after}\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            json_text = res.json()\n",
    "            posts.extend(json_text[\"data\"][\"children\"])\n",
    "            after = json_text[\"data\"][\"after\"]\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(0.2)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Calling the function to extract posts (first level of crawling)\n",
    "posts = extract_posts(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data from the first layer of crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the key information crawled from the first layer of crawling\n",
    "# In order to save the text in csv file properly, non-characters are replaced by spaces\n",
    "def trim_posts(posts):\n",
    "\n",
    "    url_root = \"https://www.reddit.com\"\n",
    "\n",
    "    posts_trimmed= []\n",
    "    for i in range(len(posts)):\n",
    "        new_post = {}\n",
    "        new_post[\"name\"] = posts[i]['data'][\"name\"]\n",
    "        new_post[\"title\"] = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', posts[i]['data'][\"title\"])\n",
    "        new_post[\"url_comments\"] = url_root + posts[i]['data']['permalink']\n",
    "        posts_trimmed.append(new_post)\n",
    "    \n",
    "    return posts_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_trimmed = trim_posts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 't3_kwqigy',\n",
       " 'title': 'House reaches enough votes to impeach President Trump for the second time',\n",
       " 'url_comments': 'https://www.reddit.com/r/democrats/comments/kwqigy/house_reaches_enough_votes_to_impeach_president/'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_trimmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results of first level of crawling in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.DataFrame(posts_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>url_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_kwqigy</td>\n",
       "      <td>House reaches enough votes to impeach Presiden...</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_kwxf3v</td>\n",
       "      <td>Aged like milk x2</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_kx07lk</td>\n",
       "      <td>Classic</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_kwjpk7</td>\n",
       "      <td>Republicans Actual Argument Against Impeachmen...</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_kwz1uh</td>\n",
       "      <td>about sums it up</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                              title  \\\n",
       "0  t3_kwqigy  House reaches enough votes to impeach Presiden...   \n",
       "1  t3_kwxf3v                                  Aged like milk x2   \n",
       "2  t3_kx07lk                                         Classic      \n",
       "3  t3_kwjpk7  Republicans Actual Argument Against Impeachmen...   \n",
       "4  t3_kwz1uh                                   about sums it up   \n",
       "\n",
       "                                        url_comments  \n",
       "0  https://www.reddit.com/r/democrats/comments/kw...  \n",
       "1  https://www.reddit.com/r/democrats/comments/kw...  \n",
       "2  https://www.reddit.com/r/democrats/comments/kx...  \n",
       "3  https://www.reddit.com/r/democrats/comments/kw...  \n",
       "4  https://www.reddit.com/r/democrats/comments/kw...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv(\"../data/democrats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading csv files with permalinks for second level of crawling - extract comments under the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_csv(\"../data/democrats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>url_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_kv4lr7</td>\n",
       "      <td>House Democrats launch second impeachment of T...</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_kvg3xu</td>\n",
       "      <td>Do I have to</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_kv4oca</td>\n",
       "      <td>Camp Auschwitz  guy identified</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_kvekkt</td>\n",
       "      <td>No Crawling Back</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_kvfqwa</td>\n",
       "      <td>Use the 14th Amendment to ban Trump</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                              title  \\\n",
       "0  t3_kv4lr7  House Democrats launch second impeachment of T...   \n",
       "1  t3_kvg3xu                                      Do I have to    \n",
       "2  t3_kv4oca                    Camp Auschwitz  guy identified    \n",
       "3  t3_kvekkt                                No Crawling Back      \n",
       "4  t3_kvfqwa                Use the 14th Amendment to ban Trump   \n",
       "\n",
       "                                        url_comments  \n",
       "0  https://www.reddit.com/r/democrats/comments/kv...  \n",
       "1  https://www.reddit.com/r/democrats/comments/kv...  \n",
       "2  https://www.reddit.com/r/democrats/comments/kv...  \n",
       "3  https://www.reddit.com/r/democrats/comments/kv...  \n",
       "4  https://www.reddit.com/r/democrats/comments/kv...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns to store selftext and comments\n",
    "df_posts[\"comments\"] = \"\"\n",
    "df_posts[\"selftext\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>url_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_kv4lr7</td>\n",
       "      <td>House Democrats launch second impeachment of T...</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_kvg3xu</td>\n",
       "      <td>Do I have to</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_kv4oca</td>\n",
       "      <td>Camp Auschwitz  guy identified</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_kvekkt</td>\n",
       "      <td>No Crawling Back</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_kvfqwa</td>\n",
       "      <td>Use the 14th Amendment to ban Trump</td>\n",
       "      <td>https://www.reddit.com/r/democrats/comments/kv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                              title  \\\n",
       "0  t3_kv4lr7  House Democrats launch second impeachment of T...   \n",
       "1  t3_kvg3xu                                      Do I have to    \n",
       "2  t3_kv4oca                    Camp Auschwitz  guy identified    \n",
       "3  t3_kvekkt                                No Crawling Back      \n",
       "4  t3_kvfqwa                Use the 14th Amendment to ban Trump   \n",
       "\n",
       "                                        url_comments comments selftext  \n",
       "0  https://www.reddit.com/r/democrats/comments/kv...                    \n",
       "1  https://www.reddit.com/r/democrats/comments/kv...                    \n",
       "2  https://www.reddit.com/r/democrats/comments/kv...                    \n",
       "3  https://www.reddit.com/r/democrats/comments/kv...                    \n",
       "4  https://www.reddit.com/r/democrats/comments/kv...                    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract selftext and comments based on permalinks to the \"comments\" page\n",
    "\n",
    "def extract_selftext_comments(url):\n",
    "    headers = {\"User-agent\": \"Bot DSI\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "    if (res.status_code == 200):\n",
    "        json_text = res.json()\n",
    "\n",
    "        comments_array = []\n",
    "        selftext = \"\"\n",
    "        \n",
    "        # gather comments and convert it to a string\n",
    "        for i in range(len(json_text[1][\"data\"][\"children\"])):\n",
    "            try:\n",
    "                comments_array.append(json_text[1][\"data\"][\"children\"][i][\"data\"][\"body\"])\n",
    "            except:\n",
    "                pass\n",
    "        #selftext does not necessarily exist - skipped if not available\n",
    "        try:\n",
    "            selftext.append(json_text[0][\"data\"][\"children\"][0][\"data\"][\"selftext\"])\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            selftext.append(json_text[0][\"data\"][\"children\"][0][\"data\"][\"crosspost_parent_list\"][0][\"selftext\"])\n",
    "        except:\n",
    "                pass\n",
    "    \n",
    "    output = {\"selftext\": selftext, \"comments\":\" \".join(comments_array)}\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the data from second level of crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the non-characters in order to save text in csv files properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_posts.index:\n",
    "    print(i)\n",
    "    url = df_posts.loc[i, \"url_comments\"][0:-1] + \".json\"\n",
    "    print(url)\n",
    "    selftext_comments = extract_selftext_comments(url)\n",
    "    selftext = selftext_comments[\"selftext\"]\n",
    "    comments = selftext_comments[\"comments\"]\n",
    "    selftext = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', selftext)\n",
    "    comments = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', comments)\n",
    "    df_posts.loc[i, \"selftext\"] = selftext\n",
    "    df_posts.loc[i, \"comments\"] = comments\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the information from second layer of crawling - ~1000 posts and related comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv(\"../data/democrats_comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
